{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/beyond-tbb-code/blob/main/backgrounds/sgxl-clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K74zxktiQyub"
      },
      "source": [
        "# **StyleGANXL + CLIP 🖼️**\n",
        "\n",
        "## Generate images from text prompts using StyleGANXL with CLIP guidance.\n",
        "\n",
        "(Modified by Katherine Crowson to optimize in W+ space)\n",
        "\n",
        "This notebook is a work in progress, head over [here](https://github.com/CasualGANPapers/unconditional-StyleGAN-CLIP) if you want to be up to date with its changes.\n",
        "\n",
        "Largely based on code by  [Katherine Crowson](https://github.com/crowsonkb) and [nshepperd](https://github.com/nshepperd).\n",
        "\n",
        "Mostly made possible because of [StyleGAN-XL](https://github.com/autonomousvision/stylegan_xl) and [CLIP](https://github.com/openai/CLIP).\n",
        "\n",
        "Created by [Eugenio Herrera](https://github.com/ouhenio) and [Rodrigo Mello](https://github.com/ryudrigo).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "!pip install timm==0.4.12"
      ],
      "metadata": {
        "id": "pxYnuw7nJDo4",
        "outputId": "20e773df-5ed7-4059-e83c-b0ef32bbdbdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dill\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.8\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm==0.4.12)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i6Ri2kT3N3Gc",
        "cellView": "form",
        "outputId": "729d3c3d-eb66-494b-f8a8-1dcca7181cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan_xl'...\n",
            "remote: Enumerating objects: 298, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 298 (delta 74), reused 51 (delta 51), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (298/298), 13.88 MiB | 31.23 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 251, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 251 (delta 3), reused 3 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (251/251), 8.93 MiB | 7.83 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "Obtaining file:///content/CLIP\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.4.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.2.0\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, einops\n",
            "Successfully installed einops-0.7.0 ninja-1.11.1.1\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm) (12.4.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n",
            "100%|███████████████████████████████████████| 335M/335M [00:26<00:00, 13.2MiB/s]\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Install libraries and define functions** 🏗️🛠️\n",
        "# @markdown This cell will take a little while because it has to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!git clone https://github.com/autonomousvision/stylegan_xl\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!pip install timm\n",
        "\n",
        "## I'll probably have to trim stuff here\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan_xl')\n",
        "\n",
        "import io\n",
        "import os, time, glob\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)\n",
        "\n",
        "# Functions (many must be trimmed too)\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    !wget -c '{url_or_path}'\n",
        "\n",
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    if len(targets) == 1: # Keeps consitent results vs previous method for single objective guidance\n",
        "      return loss(x, targets[0])\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    return torch.stack(distances, dim=-1).sum(dim=-1)\n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "def embed_url(url):\n",
        "  image = Image.open(fetch(url)).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/16\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "\n",
        "clip_model = CLIP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "guZeRGM6OmaU",
        "outputId": "709dde50-5c72-4e68-9c7a-67d8bc348c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-16 14:57:45--  https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet64.pkl\n",
            "Resolving s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)... 52.219.208.17, 52.219.47.119, 52.219.170.157, ...\n",
            "Connecting to s3.eu-central-1.amazonaws.com (s3.eu-central-1.amazonaws.com)|52.219.208.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1701610318 (1.6G) [binary/octet-stream]\n",
            "Saving to: ‘imagenet64.pkl’\n",
            "\n",
            "imagenet64.pkl      100%[===================>]   1.58G  51.4MB/s    in 34s     \n",
            "\n",
            "2024-03-16 14:58:19 (48.2 MB/s) - ‘imagenet64.pkl’ saved [1701610318/1701610318]\n",
            "\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Model selection** 🎭\n",
        "\n",
        "Model = 'Imagenet-64' #@param [\"Imagenet-64\", \"Imagenet\", \"Pokemon\", \"FFHQ\"]\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "network_url = {\n",
        "    \"Imagenet-64\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet64.pkl\",\n",
        "    \"Imagenet\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/imagenet512.pkl\",\n",
        "    \"Pokemon\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon256.pkl\",\n",
        "    \"FFHQ\": \"https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/ffhq256.pkl\"\n",
        "}\n",
        "\n",
        "network_name = network_url[Model].split(\"/\")[-1]\n",
        "fetch_model(network_url[Model])\n",
        "\n",
        "with dnnlib.util.open_url(network_name) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "cs = torch.zeros([10000, G.mapping.c_dim], device=device)\n",
        "for i in range(cs.shape[0]):\n",
        "  cs[i,i//10]=1\n",
        "w_stds = G.mapping(zs, cs)\n",
        "w_stds = w_stds.reshape(10, 1000, G.num_ws, -1)\n",
        "w_stds=w_stds.std(0).mean(0)[0]\n",
        "w_all_classes_avg = G.mapping.w_avg.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAknegrdPM-d",
        "outputId": "a3ac81ce-a92a-4bd1-9f22-be571a6d83c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... "
          ]
        }
      ],
      "source": [
        "#@markdown #**Run the model** 🚀\n",
        "#@markdown `texts`: Enter here a prompt to guide the image generation. You can enter more than one prompt separated with\n",
        "#@markdown `|`, which will cause the guidance to focus on the different prompts at the same time, allowing to mix and play\n",
        "#@markdown with the generation process.\n",
        "\n",
        "#@markdown `steps`: Number of optimization steps. The more steps, the longer it will try to generate an image relevant to the prompt.\n",
        "\n",
        "#@markdown `seed`: Determines the randomness seed. Using the same seed and prompt should give you similar results at every run.\n",
        "#@markdown Use `-1` for a random seed.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "texts = \"a park in the evening\"#@param {type:\"string\"}\n",
        "steps = 100#@param {type:\"number\"}\n",
        "seed = 9#@param {type:\"number\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "if seed == -1:\n",
        "    seed = np.random.randint(0,9e9)\n",
        "    print(f\"Your random seed is: {seed}\")\n",
        "\n",
        "texts = [frase.strip() for frase in texts.split(\"|\") if frase]\n",
        "\n",
        "targets = [clip_model.embed_text(text) for text in texts]\n",
        "\n",
        "\n",
        "'''\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "one_hot_class = torch.zeros(1000)\n",
        "initial_class = torch.randint(0,1000, (1,))[0]\n",
        "one_hot_class[initial_class]=1\n",
        "one_hot_class = one_hot_class.repeat((10000, 1))\n",
        "cs = one_hot_class.to(device)\n",
        "w_stds = G.mapping(zs, cs)\n",
        "w_stds=w_stds.std(0)[0]\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf = Compose([\n",
        "  # Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "])\n",
        "\n",
        "initial_batch=4 #actually that will be multiplied by initial_image_steps\n",
        "initial_image_steps=32\n",
        "'''\n",
        "c = torch.zeros((1000)) #just to pick a closer initial image\n",
        "c[initial_class]=1\n",
        "c = c.repeat(initial_batch, 1)\n",
        "c=c.to(device)\n",
        "'''\n",
        "def run(timestring):\n",
        "  torch.manual_seed(seed)\n",
        "  with torch.no_grad():\n",
        "    qs = []\n",
        "    losses = []\n",
        "    for _ in range(initial_image_steps):\n",
        "      a = torch.randn([initial_batch, 512], device=device)*0.4 + w_stds*0.4\n",
        "      q = ((a-w_all_classes_avg)/w_stds)\n",
        "      images = G.synthesis((q * w_stds + w_all_classes_avg).unsqueeze(1).repeat([1, G.num_ws, 1]))\n",
        "      embeds = embed_image(images.add(1).div(2))\n",
        "      loss = prompts_dist_loss(embeds, targets, spherical_dist_loss).mean(0)\n",
        "      i = torch.argmin(loss)\n",
        "      qs.append(q[i])\n",
        "      losses.append(loss[i])\n",
        "    qs = torch.stack(qs)\n",
        "    losses = torch.stack(losses)\n",
        "    # print(losses)\n",
        "    # print(losses.shape, qs.shape)\n",
        "    i = torch.argmin(losses)\n",
        "    q = qs[i].unsqueeze(0).repeat([G.num_ws, 1]).requires_grad_()\n",
        "\n",
        "\n",
        "  # Sampling loop\n",
        "  q_ema = q\n",
        "  print(q.shape)\n",
        "  opt = torch.optim.AdamW([q], lr=0.05, betas=(0., 0.999), weight_decay=0.025)\n",
        "  loop = tqdm(range(steps))\n",
        "  for i in loop:\n",
        "    opt.zero_grad()\n",
        "    w = q * w_stds\n",
        "    image = G.synthesis((q * w_stds + w_all_classes_avg)[None], noise_mode='const')\n",
        "    embed = embed_image(image.add(1).div(2))\n",
        "    loss = prompts_dist_loss(embed, targets, spherical_dist_loss).mean()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "    q_ema = q_ema * 0.98 + q * 0.02\n",
        "    image = G.synthesis((q_ema * w_stds + w_all_classes_avg)[None], noise_mode='const')\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      display(TF.to_pil_image(tf(image)[0]))\n",
        "      print(f\"Image {i}/{steps} | Current loss: {loss}\")\n",
        "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "    os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n",
        "\n",
        "try:\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  run(timestring)\n",
        "except KeyboardInterrupt:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls samples/*/0300.jpg\n",
        "# !ls samples/{timestring}/*.jpg"
      ],
      "metadata": {
        "id": "zi-j_mqCBP1r",
        "outputId": "7fdad2d4-cb98-478f-9ed8-375e6798bc57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples/20240316135756/0000.jpg  samples/20240316135756/0100.jpg  samples/20240316135756/0200.jpg\n",
            "samples/20240316135756/0001.jpg  samples/20240316135756/0101.jpg  samples/20240316135756/0201.jpg\n",
            "samples/20240316135756/0002.jpg  samples/20240316135756/0102.jpg  samples/20240316135756/0202.jpg\n",
            "samples/20240316135756/0003.jpg  samples/20240316135756/0103.jpg  samples/20240316135756/0203.jpg\n",
            "samples/20240316135756/0004.jpg  samples/20240316135756/0104.jpg  samples/20240316135756/0204.jpg\n",
            "samples/20240316135756/0005.jpg  samples/20240316135756/0105.jpg  samples/20240316135756/0205.jpg\n",
            "samples/20240316135756/0006.jpg  samples/20240316135756/0106.jpg  samples/20240316135756/0206.jpg\n",
            "samples/20240316135756/0007.jpg  samples/20240316135756/0107.jpg  samples/20240316135756/0207.jpg\n",
            "samples/20240316135756/0008.jpg  samples/20240316135756/0108.jpg  samples/20240316135756/0208.jpg\n",
            "samples/20240316135756/0009.jpg  samples/20240316135756/0109.jpg  samples/20240316135756/0209.jpg\n",
            "samples/20240316135756/0010.jpg  samples/20240316135756/0110.jpg  samples/20240316135756/0210.jpg\n",
            "samples/20240316135756/0011.jpg  samples/20240316135756/0111.jpg  samples/20240316135756/0211.jpg\n",
            "samples/20240316135756/0012.jpg  samples/20240316135756/0112.jpg  samples/20240316135756/0212.jpg\n",
            "samples/20240316135756/0013.jpg  samples/20240316135756/0113.jpg  samples/20240316135756/0213.jpg\n",
            "samples/20240316135756/0014.jpg  samples/20240316135756/0114.jpg  samples/20240316135756/0214.jpg\n",
            "samples/20240316135756/0015.jpg  samples/20240316135756/0115.jpg  samples/20240316135756/0215.jpg\n",
            "samples/20240316135756/0016.jpg  samples/20240316135756/0116.jpg  samples/20240316135756/0216.jpg\n",
            "samples/20240316135756/0017.jpg  samples/20240316135756/0117.jpg  samples/20240316135756/0217.jpg\n",
            "samples/20240316135756/0018.jpg  samples/20240316135756/0118.jpg  samples/20240316135756/0218.jpg\n",
            "samples/20240316135756/0019.jpg  samples/20240316135756/0119.jpg  samples/20240316135756/0219.jpg\n",
            "samples/20240316135756/0020.jpg  samples/20240316135756/0120.jpg  samples/20240316135756/0220.jpg\n",
            "samples/20240316135756/0021.jpg  samples/20240316135756/0121.jpg  samples/20240316135756/0221.jpg\n",
            "samples/20240316135756/0022.jpg  samples/20240316135756/0122.jpg  samples/20240316135756/0222.jpg\n",
            "samples/20240316135756/0023.jpg  samples/20240316135756/0123.jpg  samples/20240316135756/0223.jpg\n",
            "samples/20240316135756/0024.jpg  samples/20240316135756/0124.jpg  samples/20240316135756/0224.jpg\n",
            "samples/20240316135756/0025.jpg  samples/20240316135756/0125.jpg  samples/20240316135756/0225.jpg\n",
            "samples/20240316135756/0026.jpg  samples/20240316135756/0126.jpg  samples/20240316135756/0226.jpg\n",
            "samples/20240316135756/0027.jpg  samples/20240316135756/0127.jpg  samples/20240316135756/0227.jpg\n",
            "samples/20240316135756/0028.jpg  samples/20240316135756/0128.jpg  samples/20240316135756/0228.jpg\n",
            "samples/20240316135756/0029.jpg  samples/20240316135756/0129.jpg  samples/20240316135756/0229.jpg\n",
            "samples/20240316135756/0030.jpg  samples/20240316135756/0130.jpg  samples/20240316135756/0230.jpg\n",
            "samples/20240316135756/0031.jpg  samples/20240316135756/0131.jpg  samples/20240316135756/0231.jpg\n",
            "samples/20240316135756/0032.jpg  samples/20240316135756/0132.jpg  samples/20240316135756/0232.jpg\n",
            "samples/20240316135756/0033.jpg  samples/20240316135756/0133.jpg  samples/20240316135756/0233.jpg\n",
            "samples/20240316135756/0034.jpg  samples/20240316135756/0134.jpg  samples/20240316135756/0234.jpg\n",
            "samples/20240316135756/0035.jpg  samples/20240316135756/0135.jpg  samples/20240316135756/0235.jpg\n",
            "samples/20240316135756/0036.jpg  samples/20240316135756/0136.jpg  samples/20240316135756/0236.jpg\n",
            "samples/20240316135756/0037.jpg  samples/20240316135756/0137.jpg  samples/20240316135756/0237.jpg\n",
            "samples/20240316135756/0038.jpg  samples/20240316135756/0138.jpg  samples/20240316135756/0238.jpg\n",
            "samples/20240316135756/0039.jpg  samples/20240316135756/0139.jpg  samples/20240316135756/0239.jpg\n",
            "samples/20240316135756/0040.jpg  samples/20240316135756/0140.jpg  samples/20240316135756/0240.jpg\n",
            "samples/20240316135756/0041.jpg  samples/20240316135756/0141.jpg  samples/20240316135756/0241.jpg\n",
            "samples/20240316135756/0042.jpg  samples/20240316135756/0142.jpg  samples/20240316135756/0242.jpg\n",
            "samples/20240316135756/0043.jpg  samples/20240316135756/0143.jpg  samples/20240316135756/0243.jpg\n",
            "samples/20240316135756/0044.jpg  samples/20240316135756/0144.jpg  samples/20240316135756/0244.jpg\n",
            "samples/20240316135756/0045.jpg  samples/20240316135756/0145.jpg  samples/20240316135756/0245.jpg\n",
            "samples/20240316135756/0046.jpg  samples/20240316135756/0146.jpg  samples/20240316135756/0246.jpg\n",
            "samples/20240316135756/0047.jpg  samples/20240316135756/0147.jpg  samples/20240316135756/0247.jpg\n",
            "samples/20240316135756/0048.jpg  samples/20240316135756/0148.jpg  samples/20240316135756/0248.jpg\n",
            "samples/20240316135756/0049.jpg  samples/20240316135756/0149.jpg  samples/20240316135756/0249.jpg\n",
            "samples/20240316135756/0050.jpg  samples/20240316135756/0150.jpg  samples/20240316135756/0250.jpg\n",
            "samples/20240316135756/0051.jpg  samples/20240316135756/0151.jpg  samples/20240316135756/0251.jpg\n",
            "samples/20240316135756/0052.jpg  samples/20240316135756/0152.jpg  samples/20240316135756/0252.jpg\n",
            "samples/20240316135756/0053.jpg  samples/20240316135756/0153.jpg  samples/20240316135756/0253.jpg\n",
            "samples/20240316135756/0054.jpg  samples/20240316135756/0154.jpg  samples/20240316135756/0254.jpg\n",
            "samples/20240316135756/0055.jpg  samples/20240316135756/0155.jpg  samples/20240316135756/0255.jpg\n",
            "samples/20240316135756/0056.jpg  samples/20240316135756/0156.jpg  samples/20240316135756/0256.jpg\n",
            "samples/20240316135756/0057.jpg  samples/20240316135756/0157.jpg  samples/20240316135756/0257.jpg\n",
            "samples/20240316135756/0058.jpg  samples/20240316135756/0158.jpg  samples/20240316135756/0258.jpg\n",
            "samples/20240316135756/0059.jpg  samples/20240316135756/0159.jpg  samples/20240316135756/0259.jpg\n",
            "samples/20240316135756/0060.jpg  samples/20240316135756/0160.jpg  samples/20240316135756/0260.jpg\n",
            "samples/20240316135756/0061.jpg  samples/20240316135756/0161.jpg  samples/20240316135756/0261.jpg\n",
            "samples/20240316135756/0062.jpg  samples/20240316135756/0162.jpg  samples/20240316135756/0262.jpg\n",
            "samples/20240316135756/0063.jpg  samples/20240316135756/0163.jpg  samples/20240316135756/0263.jpg\n",
            "samples/20240316135756/0064.jpg  samples/20240316135756/0164.jpg  samples/20240316135756/0264.jpg\n",
            "samples/20240316135756/0065.jpg  samples/20240316135756/0165.jpg  samples/20240316135756/0265.jpg\n",
            "samples/20240316135756/0066.jpg  samples/20240316135756/0166.jpg  samples/20240316135756/0266.jpg\n",
            "samples/20240316135756/0067.jpg  samples/20240316135756/0167.jpg  samples/20240316135756/0267.jpg\n",
            "samples/20240316135756/0068.jpg  samples/20240316135756/0168.jpg  samples/20240316135756/0268.jpg\n",
            "samples/20240316135756/0069.jpg  samples/20240316135756/0169.jpg  samples/20240316135756/0269.jpg\n",
            "samples/20240316135756/0070.jpg  samples/20240316135756/0170.jpg  samples/20240316135756/0270.jpg\n",
            "samples/20240316135756/0071.jpg  samples/20240316135756/0171.jpg  samples/20240316135756/0271.jpg\n",
            "samples/20240316135756/0072.jpg  samples/20240316135756/0172.jpg  samples/20240316135756/0272.jpg\n",
            "samples/20240316135756/0073.jpg  samples/20240316135756/0173.jpg  samples/20240316135756/0273.jpg\n",
            "samples/20240316135756/0074.jpg  samples/20240316135756/0174.jpg  samples/20240316135756/0274.jpg\n",
            "samples/20240316135756/0075.jpg  samples/20240316135756/0175.jpg  samples/20240316135756/0275.jpg\n",
            "samples/20240316135756/0076.jpg  samples/20240316135756/0176.jpg  samples/20240316135756/0276.jpg\n",
            "samples/20240316135756/0077.jpg  samples/20240316135756/0177.jpg  samples/20240316135756/0277.jpg\n",
            "samples/20240316135756/0078.jpg  samples/20240316135756/0178.jpg  samples/20240316135756/0278.jpg\n",
            "samples/20240316135756/0079.jpg  samples/20240316135756/0179.jpg  samples/20240316135756/0279.jpg\n",
            "samples/20240316135756/0080.jpg  samples/20240316135756/0180.jpg  samples/20240316135756/0280.jpg\n",
            "samples/20240316135756/0081.jpg  samples/20240316135756/0181.jpg  samples/20240316135756/0281.jpg\n",
            "samples/20240316135756/0082.jpg  samples/20240316135756/0182.jpg  samples/20240316135756/0282.jpg\n",
            "samples/20240316135756/0083.jpg  samples/20240316135756/0183.jpg  samples/20240316135756/0283.jpg\n",
            "samples/20240316135756/0084.jpg  samples/20240316135756/0184.jpg  samples/20240316135756/0284.jpg\n",
            "samples/20240316135756/0085.jpg  samples/20240316135756/0185.jpg  samples/20240316135756/0285.jpg\n",
            "samples/20240316135756/0086.jpg  samples/20240316135756/0186.jpg  samples/20240316135756/0286.jpg\n",
            "samples/20240316135756/0087.jpg  samples/20240316135756/0187.jpg  samples/20240316135756/0287.jpg\n",
            "samples/20240316135756/0088.jpg  samples/20240316135756/0188.jpg  samples/20240316135756/0288.jpg\n",
            "samples/20240316135756/0089.jpg  samples/20240316135756/0189.jpg  samples/20240316135756/0289.jpg\n",
            "samples/20240316135756/0090.jpg  samples/20240316135756/0190.jpg  samples/20240316135756/0290.jpg\n",
            "samples/20240316135756/0091.jpg  samples/20240316135756/0191.jpg  samples/20240316135756/0291.jpg\n",
            "samples/20240316135756/0092.jpg  samples/20240316135756/0192.jpg  samples/20240316135756/0292.jpg\n",
            "samples/20240316135756/0093.jpg  samples/20240316135756/0193.jpg  samples/20240316135756/0293.jpg\n",
            "samples/20240316135756/0094.jpg  samples/20240316135756/0194.jpg  samples/20240316135756/0294.jpg\n",
            "samples/20240316135756/0095.jpg  samples/20240316135756/0195.jpg  samples/20240316135756/0295.jpg\n",
            "samples/20240316135756/0096.jpg  samples/20240316135756/0196.jpg  samples/20240316135756/0296.jpg\n",
            "samples/20240316135756/0097.jpg  samples/20240316135756/0197.jpg  samples/20240316135756/0297.jpg\n",
            "samples/20240316135756/0098.jpg  samples/20240316135756/0198.jpg  samples/20240316135756/0298.jpg\n",
            "samples/20240316135756/0099.jpg  samples/20240316135756/0199.jpg  samples/20240316135756/0299.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display(Image.open('samples/20240307230402/0500.jpg'))"
      ],
      "metadata": {
        "id": "pOudXXDMBYde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ffmpeg -r 60 -i 'samples/20240307233800/%04d.jpg' -vcodec libx264 -crf 18 -pix_fmt yuv420p a-seed-13.mp4\n",
        "!ffmpeg -r 60 -i samples/{timestring}/%04d.jpg -vcodec libx264 -crf 18 -pix_fmt yuv420p dogpark-{seed}-{steps}.mp4"
      ],
      "metadata": {
        "id": "R3T08C5FBsRN",
        "outputId": "3c620941-942a-4b2e-93ae-1dd1d8f3d646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, image2, from 'samples/20240316140633/%04d.jpg':\n",
            "  Duration: 00:00:20.00, start: 0.000000, bitrate: N/A\n",
            "  Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 512x512 [SAR 1:1 DAR 1:1], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x556d94b50880] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0m\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0musing SAR=1/1\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mprofile High, level 3.1, 4:2:0, 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0m264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=18.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'dogpark-9-500.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt470bg/unknown/unknown, progressive), 512x512 [SAR 1:1 DAR 1:1], q=2-31, 60 fps, 15360 tbn\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
            "frame=  501 fps= 35 q=-1.0 Lsize=    4999kB time=00:00:08.30 bitrate=4934.1kbits/s dup=1 drop=0 speed=0.575x    \n",
            "video:4992kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.135911%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mframe I:3     Avg QP:22.07  size: 39137\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mframe P:131   Avg QP:22.11  size: 23126\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mframe B:367   Avg QP:25.92  size:  5353\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mconsecutive B-frames:  1.0%  3.6%  1.2% 94.2%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mmb I  I16..4:  1.4% 96.5%  2.1%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mmb P  I16..4:  2.7% 31.3%  0.6%  P16..4: 25.9% 23.4% 14.5%  0.0%  0.0%    skip: 1.7%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mmb B  I16..4:  0.5%  1.9%  0.0%  B16..8: 40.6%  8.8%  2.1%  direct: 5.7%  skip:40.5%  L0:28.0% L1:33.3% BI:38.7%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0m8x8 transform intra:89.1% inter:79.9%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mcoded y,uvDC,uvAC intra: 72.7% 81.0% 30.1% inter: 29.0% 28.6% 3.6%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mi16 v,h,dc,p: 17% 66%  7% 10%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 32% 24%  2%  2%  3%  3%  4%  4%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 19% 24% 14%  6%  7%  8%  7%  9%  7%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mi8c dc,h,v,p: 40% 33% 20%  7%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mWeighted P-Frames: Y:33.6% UV:14.5%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mref P L0: 54.1% 24.5% 17.7%  3.2%  0.6%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mref B L0: 95.3%  4.1%  0.5%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mref B L1: 98.9%  1.1%\n",
            "\u001b[1;36m[libx264 @ 0x556d94974cc0] \u001b[0mkb/s:4897.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DmpNBbPeB61c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}